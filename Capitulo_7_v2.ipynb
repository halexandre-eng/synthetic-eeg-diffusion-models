{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de Canais Sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Difusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando o modelo para prever o canal F3 usando ['AF4', 'F8']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 6.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Loss: 3.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Loss: 2.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Loss: 1.7406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200, Loss: 1.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200, Loss: 1.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Loss: 1.0950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200, Loss: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200, Loss: 0.8077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200, Loss: 0.6617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Loss: 0.6112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Loss: 0.5659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200, Loss: 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200, Loss: 0.4854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200, Loss: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200, Loss: 0.4415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200, Loss: 0.4071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Loss: 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Loss: 0.3580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200, Loss: 0.3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200, Loss: 0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200, Loss: 0.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Loss: 0.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200, Loss: 0.3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200, Loss: 0.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200, Loss: 0.2934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200, Loss: 0.2907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200, Loss: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Loss: 0.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200, Loss: 0.2758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200, Loss: 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200, Loss: 0.2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200, Loss: 0.2636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200, Loss: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200, Loss: 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200, Loss: 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200, Loss: 0.2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200, Loss: 0.2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Loss: 0.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200, Loss: 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200, Loss: 0.2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200, Loss: 0.2348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200, Loss: 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200, Loss: 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200, Loss: 0.2337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200, Loss: 0.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200, Loss: 0.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200, Loss: 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Loss: 0.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200, Loss: 0.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200, Loss: 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200, Loss: 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200, Loss: 0.2174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200, Loss: 0.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200, Loss: 0.2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200, Loss: 0.2042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200, Loss: 0.2084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200, Loss: 0.2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200, Loss: 0.2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200, Loss: 0.1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200, Loss: 0.1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200, Loss: 0.1926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200, Loss: 0.1954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200, Loss: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200, Loss: 0.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/200, Loss: 0.2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/200, Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200, Loss: 0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200, Loss: 0.1886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200, Loss: 0.2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200, Loss: 0.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200, Loss: 0.1928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200, Loss: 0.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200, Loss: 0.1921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200, Loss: 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200, Loss: 0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200, Loss: 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200, Loss: 0.1877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200, Loss: 0.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200, Loss: 0.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200, Loss: 0.1807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200, Loss: 0.1779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200, Loss: 0.1859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200, Loss: 0.1883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/200, Loss: 0.1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200, Loss: 0.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200, Loss: 0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200, Loss: 0.1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Loss: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200, Loss: 0.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200, Loss: 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200, Loss: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200, Loss: 0.1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200, Loss: 0.1829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200, Loss: 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200, Loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200, Loss: 0.1805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200, Loss: 0.1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200, Loss: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200, Loss: 0.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200, Loss: 0.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200, Loss: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200, Loss: 0.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200, Loss: 0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200, Loss: 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200, Loss: 0.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200, Loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200, Loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200, Loss: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200, Loss: 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200, Loss: 0.1677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200, Loss: 0.1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200, Loss: 0.1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200, Loss: 0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200, Loss: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200, Loss: 0.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200, Loss: 0.1763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200, Loss: 0.1653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200, Loss: 0.1778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200, Loss: 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200, Loss: 0.1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200, Loss: 0.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200, Loss: 0.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200, Loss: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200, Loss: 0.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200, Loss: 0.1758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/200, Loss: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200, Loss: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200, Loss: 0.1809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200, Loss: 0.1629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/200, Loss: 0.1707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200, Loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/200, Loss: 0.1713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/200, Loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200, Loss: 0.1621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200, Loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/200, Loss: 0.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200, Loss: 0.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200, Loss: 0.1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200, Loss: 0.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200, Loss: 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/200, Loss: 0.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200, Loss: 0.1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200, Loss: 0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200, Loss: 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200, Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200, Loss: 0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200, Loss: 0.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200, Loss: 0.1765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200, Loss: 0.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/200, Loss: 0.1681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200, Loss: 0.1653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200, Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200, Loss: 0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/200, Loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200, Loss: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/200, Loss: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200, Loss: 0.1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200, Loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200, Loss: 0.1670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200, Loss: 0.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200, Loss: 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/200, Loss: 0.1635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200, Loss: 0.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/200, Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200, Loss: 0.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200, Loss: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200, Loss: 0.1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200, Loss: 0.1747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200, Loss: 0.1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200, Loss: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200, Loss: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200, Loss: 0.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200, Loss: 0.1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/200, Loss: 0.1706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200, Loss: 0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200, Loss: 0.1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200, Loss: 0.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200, Loss: 0.1597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200, Loss: 0.1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/200, Loss: 0.1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200, Loss: 0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200, Loss: 0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200, Loss: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/200, Loss: 0.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/200, Loss: 0.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200, Loss: 0.1660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200, Loss: 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200, Loss: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200, Loss: 0.1655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/200, Loss: 0.1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200, Loss: 0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200, Loss: 0.1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/200, Loss: 0.1657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200, Loss: 0.1674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200, Loss: 0.1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200, Loss: 0.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200, Loss: 0.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Loss: 0.1627\n",
      "Modelo salvo como modelo_F3.pth\n",
      "MSE para F3 = 6.9567, Correlação = 0.6025\n",
      "\n",
      "Resultados da Avaliação:\n",
      "  Canal_Alvo Canais_Entrada       MSE  Correlacao\n",
      "0         F3        AF4, F8  6.956672    0.602463\n",
      "Resultados da avaliação salvos em 'avaliacao_resultados_modelo_difusao_F3.csv'\n",
      "Dados sintéticos adicionados e salvos em 'eeg_com_sintetico_modelo_difusao_F3.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurações gerais\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Função para o agendamento linear dos betas\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Função para criar embeddings de tempo\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "# Modelo U-Net aprimorado para o processo reverso com embeddings de tempo\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_feat=64, time_emb_dim=256):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "        # Embedding de tempo\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels + time_emb_dim, n_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(n_feat, n_feat * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(n_feat * 2, n_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(n_feat, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: (batch_size, in_channels, seq_length)\n",
    "        # t: (batch_size,)\n",
    "\n",
    "        # Incorporar o embedding de tempo\n",
    "        t_emb = self.time_mlp(t)  # (batch_size, time_emb_dim)\n",
    "\n",
    "        # Expandir t_emb para combinar com x\n",
    "        t_emb = t_emb.unsqueeze(-1)  # (batch_size, time_emb_dim, 1)\n",
    "        t_emb = t_emb.repeat(1, 1, x.size(-1))  # (batch_size, time_emb_dim, seq_length)\n",
    "\n",
    "        # Concatenar o embedding de tempo com x\n",
    "        x = torch.cat([x, t_emb], dim=1)  # (batch_size, in_channels + time_emb_dim, seq_length)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        return x  # Saída com shape: (batch_size, out_channels, seq_length)\n",
    "\n",
    "# Função de treinamento ajustada\n",
    "def train_diffusion_model(model, optimizer, scheduler, dataloader, timesteps, betas, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False):\n",
    "            batch_inputs = batch_inputs.to(device)     # Shape: (batch_size, num_input_channels, seq_length)\n",
    "            batch_targets = batch_targets.to(device)   # Shape: (batch_size, 1, seq_length)\n",
    "\n",
    "            batch_size = batch_inputs.size(0)\n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "            # Adicionar ruído aos alvos\n",
    "            noise = torch.randn_like(batch_targets)\n",
    "            sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "            x_t = sqrt_alphas_cumprod_t * batch_targets + sqrt_one_minus_alphas_cumprod_t * noise  # x_t para os alvos\n",
    "\n",
    "            # Concatenar entradas e x_t\n",
    "            model_input = torch.cat([batch_inputs, x_t], dim=1)  # Shape: (batch_size, in_channels, seq_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            noise_pred = model(model_input, t)\n",
    "\n",
    "            loss = nn.MSELoss()(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "# Função para gerar o canal sintético\n",
    "def sample_from_model(model, conditioning_inputs, alphas, alphas_cumprod, betas, sqrt_one_minus_alphas_cumprod, shape):\n",
    "    model.eval()\n",
    "    batch_size = shape[0]\n",
    "    seq_length = shape[2]\n",
    "\n",
    "    x = torch.randn((batch_size, 1, seq_length), device=device)\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(len(betas))):\n",
    "            t_batch = torch.tensor([t]*batch_size, device=device).long()\n",
    "            beta_t = betas[t]\n",
    "            alpha_t = alphas[t]\n",
    "            alpha_cumprod_t = alphas_cumprod[t]\n",
    "            sqrt_one_minus_alpha_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "\n",
    "            # Concatenar conditioning_inputs e x\n",
    "            model_input = torch.cat([conditioning_inputs, x], dim=1)\n",
    "\n",
    "            noise_pred = model(model_input, t_batch)\n",
    "\n",
    "            x = (1 / sqrt_alpha_t) * (x - (beta_t / sqrt_one_minus_alpha_cumprod_t) * noise_pred)\n",
    "    return x\n",
    "\n",
    "# Carregar os dados de EEG utilizando pandas\n",
    "eeg_df = pd.read_csv('bci_competition_3_v.csv')\n",
    "\n",
    "# Lista de diretrizes para criar canais sintéticos\n",
    "diretrizes = [\n",
    "    ### cenario 1\n",
    "    # {'canal_alvo': 'AF3', 'canais_entrada': ['Fp1', 'F3']},\n",
    "    # {'canal_alvo': 'AF4', 'canais_entrada': ['Fp2', 'F4']},\n",
    "    # {'canal_alvo': 'F7', 'canais_entrada': ['FC5', 'F3']},\n",
    "    # {'canal_alvo': 'F8', 'canais_entrada': ['T8', 'F4']},\n",
    "    # {'canal_alvo': 'T7', 'canais_entrada': ['C3', 'CP1']},\n",
    "    # {'canal_alvo': 'T8', 'canais_entrada': ['C4', 'CP2']},\n",
    "    # {'canal_alvo': 'Fp1', 'canais_entrada': ['AF3', 'F3']},\n",
    "    # {'canal_alvo': 'Fp2', 'canais_entrada': ['AF4', 'F4']}\n",
    "\n",
    "    ### cenario 2\n",
    "    # {'canal_alvo': 'F3', 'canais_entrada': ['AF4', 'F8']},\n",
    "    {'canal_alvo': 'F4', 'canais_entrada': ['T7', 'CP1']},\n",
    "    # {'canal_alvo': 'C3', 'canais_entrada': ['T8', 'CP2']},\n",
    "    # {'canal_alvo': 'C4', 'canais_entrada': ['T7', 'F7']},\n",
    "    # {'canal_alvo': 'CP1', 'canais_entrada': ['T7', 'P3']},\n",
    "    # {'canal_alvo': 'CP2', 'canais_entrada': ['T8', 'P4']},\n",
    "    # {'canal_alvo': 'P3', 'canais_entrada': ['CP1', 'O1']},\n",
    "    # {'canal_alvo': 'P4', 'canais_entrada': ['CP2', 'O2']},\n",
    "\n",
    "    ### cenario 3\n",
    "\n",
    "    # {'canal_alvo': 'FC3', 'canais_entrada': ['C3', 'FC1']},\n",
    "    # {'canal_alvo': 'FC4', 'canais_entrada': ['C4', 'CP1']},\n",
    "    # {'canal_alvo': 'CP3', 'canais_entrada': ['C3', 'CP2']},\n",
    "    # {'canal_alvo': 'CP4', 'canais_entrada': ['C4', 'F7']},\n",
    "    # {'canal_alvo': 'Pz', 'canais_entrada': ['Cz', 'P3']},\n",
    "    # {'canal_alvo': 'Fz', 'canais_entrada': ['Cz', 'FC1']},\n",
    "    # {'canal_alvo': 'Oz', 'canais_entrada': ['POz', 'O1']},\n",
    "    # {'canal_alvo': 'P0z', 'canais_entrada': ['Pz', 'PO3']},\n",
    "\n",
    "]\n",
    "\n",
    "# Configurar os parâmetros da difusão\n",
    "timesteps = 1000\n",
    "betas = linear_beta_schedule(timesteps).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).to(device)\n",
    "\n",
    "# Lista para armazenar as métricas de avaliação\n",
    "avaliacao_resultados = []\n",
    "\n",
    "# Iterar sobre cada diretriz\n",
    "for diretriz in diretrizes:\n",
    "    canal_alvo = diretriz['canal_alvo']\n",
    "    canais_entrada = diretriz['canais_entrada']\n",
    "    \n",
    "    print(f\"\\nTreinando o modelo para prever o canal {canal_alvo} usando {canais_entrada}\\n\")\n",
    "    \n",
    "    # Selecionar os dados dos canais de entrada e do canal alvo\n",
    "    inputs = eeg_df[canais_entrada].values\n",
    "    targets = eeg_df[[canal_alvo]].values\n",
    "\n",
    "    # Calcular a média e desvio padrão dos canais de entrada e alvo\n",
    "    mean_inputs = inputs.mean(axis=0)\n",
    "    std_inputs = inputs.std(axis=0)\n",
    "    inputs_normalizado = (inputs - mean_inputs) / std_inputs\n",
    "\n",
    "    mean_target = targets.mean(axis=0)\n",
    "    std_target = targets.std(axis=0)\n",
    "    targets_normalizado = (targets - mean_target) / std_target\n",
    "\n",
    "    # Garantir que inputs e targets tenham o mesmo número de amostras\n",
    "    min_length = min(len(inputs_normalizado), len(targets_normalizado))\n",
    "    inputs_normalizado = inputs_normalizado[:min_length]\n",
    "    targets_normalizado = targets_normalizado[:min_length]\n",
    "\n",
    "    # Preparação dos dados: Segmentar em sequências menores\n",
    "    seq_length = 128\n",
    "    num_samples = inputs_normalizado.shape[0]\n",
    "    num_segments = num_samples // seq_length\n",
    "\n",
    "    inputs_normalizado = inputs_normalizado[:num_segments * seq_length]\n",
    "    targets_normalizado = targets_normalizado[:num_segments * seq_length]\n",
    "\n",
    "    inputs_segments = inputs_normalizado.reshape(num_segments, seq_length, len(canais_entrada))\n",
    "    targets_segments = targets_normalizado.reshape(num_segments, seq_length, 1)\n",
    "\n",
    "    # Transpor para (num_segments, num_channels, seq_length)\n",
    "    inputs_segments = inputs_segments.transpose(0, 2, 1)\n",
    "    targets_segments = targets_segments.transpose(0, 2, 1)\n",
    "\n",
    "    # Criar Dataset personalizado\n",
    "    class EEGDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, targets):\n",
    "            self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            x = self.inputs[idx]\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n",
    "\n",
    "    # Criar o dataset e dataloader\n",
    "    dataset = EEGDataset(inputs_segments, targets_segments)\n",
    "    batch_size = 32\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Definir o modelo U-Net ajustado para os canais de EEG\n",
    "    time_emb_dim = 128\n",
    "    in_channels = len(canais_entrada) + 1  # Canais de entrada + canal alvo com ruído adicionado\n",
    "    out_channels = 1  # Ruído previsto para o canal alvo\n",
    "    model = UNet(in_channels=in_channels, out_channels=out_channels, n_feat=128, time_emb_dim=time_emb_dim).to(device)\n",
    "\n",
    "    # Otimizador e scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    # Treinar o modelo\n",
    "    epochs = 200\n",
    "    train_diffusion_model(model, optimizer, scheduler, dataloader, timesteps, betas, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, epochs=epochs)\n",
    "\n",
    "    # Salvar o modelo treinado\n",
    "    model_filename = f'modelo_{canal_alvo}.pth'\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    print(f\"Modelo salvo como {model_filename}\")\n",
    "\n",
    "    # Gerar o canal sintético para todos os segmentos\n",
    "    conditioning_inputs = torch.tensor(inputs_segments, dtype=torch.float32).to(device)\n",
    "\n",
    "    num_samples = conditioning_inputs.size(0)\n",
    "    seq_length = conditioning_inputs.size(2)\n",
    "    shape = (num_samples, 1, seq_length)\n",
    "\n",
    "    # Gerar o canal sintético\n",
    "    synthetic_channels = sample_from_model(model, conditioning_inputs, alphas, alphas_cumprod, betas, sqrt_one_minus_alphas_cumprod, shape=shape)\n",
    "\n",
    "    # Desnormalizar os dados\n",
    "    synthetic_channels = synthetic_channels.cpu().numpy().squeeze()  # Shape: (num_samples, seq_length)\n",
    "    synthetic_channels_desnormalizado = synthetic_channels * std_target + mean_target  # Desnormalizar\n",
    "\n",
    "    # Flatten the synthetic data\n",
    "    synthetic_channel_flat = synthetic_channels_desnormalizado.reshape(-1)\n",
    "\n",
    "    # Criar uma nova coluna no eeg_df para o canal sintético\n",
    "    column_name = f'Sintetico_{canal_alvo}'\n",
    "\n",
    "    # Garantir que o comprimento corresponda aos índices em eeg_df\n",
    "    eeg_df[column_name] = np.nan  # Inicializar a nova coluna com NaNs\n",
    "    eeg_df.loc[:len(synthetic_channel_flat)-1, column_name] = synthetic_channel_flat\n",
    "\n",
    "    # Calcular o MSE e a correlação usando todos os dados\n",
    "    original_target = targets_normalizado.reshape(-1) * std_target + mean_target\n",
    "    synthetic_target = synthetic_channel_flat\n",
    "\n",
    "    mse = mean_squared_error(original_target[:len(synthetic_target)], synthetic_target)\n",
    "    corr = np.corrcoef(original_target[:len(synthetic_target)], synthetic_target)[0, 1]\n",
    "    print(f\"MSE para {canal_alvo} = {mse:.4f}, Correlação = {corr:.4f}\")\n",
    "\n",
    "    # Armazenar as métricas de avaliação\n",
    "    avaliacao_resultados.append({\n",
    "        'Canal_Alvo': canal_alvo,\n",
    "        'Canais_Entrada': ', '.join(canais_entrada),\n",
    "        'MSE': mse,\n",
    "        'Correlacao': corr\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "avaliacao_df = pd.DataFrame(avaliacao_resultados)\n",
    "print(\"\\nResultados da Avaliação:\")\n",
    "print(avaliacao_df)\n",
    "\n",
    "# Salvar os resultados da avaliação em um arquivo CSV\n",
    "avaliacao_df.to_csv(f'avaliacao_resultados_modelo_difusao_{canal_alvo}.csv', index=False)\n",
    "print(f\"Resultados da avaliação salvos em 'avaliacao_resultados_modelo_difusao_{canal_alvo}.csv'\")\n",
    "\n",
    "# Salvar o eeg_df com os dados sintéticos\n",
    "eeg_df.to_csv(f'eeg_com_sintetico_modelo_difusao_{canal_alvo}.csv', index=False)\n",
    "print(f\"Dados sintéticos adicionados e salvos em 'eeg_com_sintetico_modelo_difusao_{canal_alvo}.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Difusão - c/ Hyperparameter Tuning e Cross-Validation - INTERESSANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurações gerais\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Função para o agendamento linear dos betas\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Função para criar embeddings de tempo\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "# Modelo U-Net aprimorado para o processo reverso com embeddings de tempo\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_feat=64, time_emb_dim=256):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "        # Embedding de tempo\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels + time_emb_dim, n_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(n_feat, n_feat * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(n_feat * 2, n_feat, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_feat),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(n_feat, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: (batch_size, in_channels, seq_length)\n",
    "        # t: (batch_size,)\n",
    "\n",
    "        # Incorporar o embedding de tempo\n",
    "        t_emb = self.time_mlp(t)  # (batch_size, time_emb_dim)\n",
    "\n",
    "        # Expandir t_emb para combinar com x\n",
    "        t_emb = t_emb.unsqueeze(-1)  # (batch_size, time_emb_dim, 1)\n",
    "        t_emb = t_emb.repeat(1, 1, x.size(-1))  # (batch_size, time_emb_dim, seq_length)\n",
    "\n",
    "        # Concatenar o embedding de tempo com x\n",
    "        x = torch.cat([x, t_emb], dim=1)  # (batch_size, in_channels + time_emb_dim, seq_length)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        return x  # Saída com shape: (batch_size, out_channels, seq_length)\n",
    "\n",
    "# Função de treinamento ajustada\n",
    "def train_diffusion_model(model, optimizer, scheduler, dataloader, timesteps, betas,\n",
    "                          sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False):\n",
    "            batch_inputs = batch_inputs.to(device)     # Shape: (batch_size, num_input_channels, seq_length)\n",
    "            batch_targets = batch_targets.to(device)   # Shape: (batch_size, 1, seq_length)\n",
    "\n",
    "            batch_size = batch_inputs.size(0)\n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "            # Adicionar ruído aos alvos\n",
    "            noise = torch.randn_like(batch_targets)\n",
    "            sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "            x_t = sqrt_alphas_cumprod_t * batch_targets + sqrt_one_minus_alphas_cumprod_t * noise  # x_t para os alvos\n",
    "\n",
    "            # Concatenar entradas e x_t\n",
    "            model_input = torch.cat([batch_inputs, x_t], dim=1)  # Shape: (batch_size, in_channels, seq_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            noise_pred = model(model_input, t)\n",
    "\n",
    "            loss = nn.MSELoss()(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "# Função para gerar o canal sintético\n",
    "def sample_from_model(model, conditioning_inputs, alphas, alphas_cumprod, betas,\n",
    "                      sqrt_one_minus_alphas_cumprod, shape):\n",
    "    model.eval()\n",
    "    batch_size = shape[0]\n",
    "    seq_length = shape[2]\n",
    "\n",
    "    x = torch.randn((batch_size, 1, seq_length), device=device)\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(len(betas))):\n",
    "            t_batch = torch.tensor([t]*batch_size, device=device).long()\n",
    "            beta_t = betas[t]\n",
    "            alpha_t = alphas[t]\n",
    "            alpha_cumprod_t = alphas_cumprod[t]\n",
    "            sqrt_one_minus_alpha_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "\n",
    "            # Concatenar conditioning_inputs e x\n",
    "            model_input = torch.cat([conditioning_inputs, x], dim=1)\n",
    "\n",
    "            noise_pred = model(model_input, t_batch)\n",
    "\n",
    "            x = (1 / sqrt_alpha_t) * (x - (beta_t / sqrt_one_minus_alpha_cumprod_t) * noise_pred)\n",
    "    return x\n",
    "\n",
    "# Carregar os dados de EEG utilizando pandas\n",
    "eeg_df = pd.read_csv('bci_competition_3_v.csv')\n",
    "\n",
    "# Lista de diretrizes para criar canais sintéticos\n",
    "diretrizes = [\n",
    "    {'canal_alvo': 'AF3', 'canais_entrada': ['Fp1', 'F3']},\n",
    "    # {'canal_alvo': 'AF4', 'canais_entrada': ['Fp2', 'F4']},\n",
    "    # {'canal_alvo': 'F7', 'canais_entrada': ['FT7', 'F3']},\n",
    "    # {'canal_alvo': 'F8', 'canais_entrada': ['FT8', 'F4']},\n",
    "    # {'canal_alvo': 'T7', 'canais_entrada': ['C3', 'TP7']},\n",
    "    # {'canal_alvo': 'T8', 'canais_entrada': ['C4', 'TP8']},\n",
    "    # {'canal_alvo': 'Fp1', 'canais_entrada': ['AF3', 'F3']},\n",
    "    # {'canal_alvo': 'Fp2', 'canais_entrada': ['AF4', 'F4']}\n",
    "]\n",
    "\n",
    "# Configurar os parâmetros da difusão\n",
    "timesteps = 1000\n",
    "betas = linear_beta_schedule(timesteps).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).to(device)\n",
    "\n",
    "# Lista para armazenar as métricas de avaliação\n",
    "avaliacao_resultados = []\n",
    "\n",
    "# Definir hiperparâmetros para ajuste\n",
    "hiperparametros = [\n",
    "    {'n_feat': 64, 'lr': 1e-4, 'batch_size': 32, 'epochs': 200},\n",
    "    {'n_feat': 128, 'lr': 1e-4, 'batch_size': 32, 'epochs': 200},\n",
    "    {'n_feat': 128, 'lr': 5e-5, 'batch_size': 32, 'epochs': 200},\n",
    "    # Adicione mais configurações conforme necessário\n",
    "]\n",
    "\n",
    "# Definir o número de folds para cross-validation\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterar sobre cada diretriz\n",
    "for diretriz in diretrizes:\n",
    "    canal_alvo = diretriz['canal_alvo']\n",
    "    canais_entrada = diretriz['canais_entrada']\n",
    "    \n",
    "    print(f\"\\nProcessando o canal {canal_alvo} usando {canais_entrada}\\n\")\n",
    "    \n",
    "    # Selecionar os dados dos canais de entrada e do canal alvo\n",
    "    inputs = eeg_df[canais_entrada].values\n",
    "    targets = eeg_df[[canal_alvo]].values\n",
    "\n",
    "    # Calcular a média e desvio padrão dos canais de entrada e alvo\n",
    "    mean_inputs = inputs.mean(axis=0)\n",
    "    std_inputs = inputs.std(axis=0)\n",
    "    inputs_normalizado = (inputs - mean_inputs) / std_inputs\n",
    "\n",
    "    mean_target = targets.mean(axis=0)\n",
    "    std_target = targets.std(axis=0)\n",
    "    targets_normalizado = (targets - mean_target) / std_target\n",
    "\n",
    "    # Garantir que inputs e targets tenham o mesmo número de amostras\n",
    "    min_length = min(len(inputs_normalizado), len(targets_normalizado))\n",
    "    inputs_normalizado = inputs_normalizado[:min_length]\n",
    "    targets_normalizado = targets_normalizado[:min_length]\n",
    "\n",
    "    # Preparação dos dados: Segmentar em sequências menores\n",
    "    seq_length = 128\n",
    "    num_samples = inputs_normalizado.shape[0]\n",
    "    num_segments = num_samples // seq_length\n",
    "\n",
    "    inputs_normalizado = inputs_normalizado[:num_segments * seq_length]\n",
    "    targets_normalizado = targets_normalizado[:num_segments * seq_length]\n",
    "\n",
    "    inputs_segments = inputs_normalizado.reshape(num_segments, seq_length, len(canais_entrada))\n",
    "    targets_segments = targets_normalizado.reshape(num_segments, seq_length, 1)\n",
    "\n",
    "    # Transpor para (num_segments, num_channels, seq_length)\n",
    "    inputs_segments = inputs_segments.transpose(0, 2, 1)\n",
    "    targets_segments = targets_segments.transpose(0, 2, 1)\n",
    "\n",
    "    # Converter para tensores\n",
    "    inputs_tensor = torch.tensor(inputs_segments, dtype=torch.float32)\n",
    "    targets_tensor = torch.tensor(targets_segments, dtype=torch.float32)\n",
    "\n",
    "    # Iterar sobre as configurações de hiperparâmetros\n",
    "    for config in hiperparametros:\n",
    "        n_feat = config['n_feat']\n",
    "        lr = config['lr']\n",
    "        batch_size = config['batch_size']\n",
    "        epochs = config['epochs']\n",
    "        \n",
    "        print(f\"\\nTreinando o modelo para prever o canal {canal_alvo} com hiperparâmetros: n_feat={n_feat}, lr={lr}, batch_size={batch_size}, epochs={epochs}\\n\")\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        # K-Fold Cross-Validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(inputs_tensor)):\n",
    "            print(f\"Fold {fold+1}/{num_folds}\")\n",
    "            \n",
    "            # Dividir os dados em treinamento e validação\n",
    "            train_inputs = inputs_tensor[train_idx]\n",
    "            train_targets = targets_tensor[train_idx]\n",
    "            val_inputs = inputs_tensor[val_idx]\n",
    "            val_targets = targets_tensor[val_idx]\n",
    "            \n",
    "            # Criar Dataset e DataLoader para treinamento\n",
    "            train_dataset = torch.utils.data.TensorDataset(train_inputs, train_targets)\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Criar Dataset e DataLoader para validação\n",
    "            val_dataset = torch.utils.data.TensorDataset(val_inputs, val_targets)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Definir o modelo U-Net ajustado para os canais de EEG\n",
    "            time_emb_dim = 128\n",
    "            in_channels = len(canais_entrada) + 1  # Canais de entrada + canal alvo com ruído adicionado\n",
    "            out_channels = 1  # Ruído previsto para o canal alvo\n",
    "            model = UNet(in_channels=in_channels, out_channels=out_channels, n_feat=n_feat, time_emb_dim=time_emb_dim).to(device)\n",
    "            \n",
    "            # Otimizador e scheduler\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "            \n",
    "            # Treinar o modelo\n",
    "            train_diffusion_model(model, optimizer, scheduler, train_dataloader, timesteps, betas,\n",
    "                                  sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, epochs=epochs)\n",
    "            \n",
    "            # Avaliação no conjunto de validação\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_targets in val_dataloader:\n",
    "                    batch_inputs = batch_inputs.to(device)\n",
    "                    batch_targets = batch_targets.to(device)\n",
    "                    batch_size = batch_inputs.size(0)\n",
    "                    t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "                    # Adicionar ruído aos alvos\n",
    "                    noise = torch.randn_like(batch_targets)\n",
    "                    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "                    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].view(batch_size, 1, 1)\n",
    "                    x_t = sqrt_alphas_cumprod_t * batch_targets + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "                    # Concatenar entradas e x_t\n",
    "                    model_input = torch.cat([batch_inputs, x_t], dim=1)\n",
    "\n",
    "                    noise_pred = model(model_input, t)\n",
    "                    loss = nn.MSELoss()(noise_pred, noise)\n",
    "                    val_losses.append(loss.item())\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "            fold_results.append(avg_val_loss)\n",
    "        \n",
    "        # Média dos resultados nos folds\n",
    "        mean_val_loss = np.mean(fold_results)\n",
    "        std_val_loss = np.std(fold_results)\n",
    "        print(f\"Média da Val Loss nos folds: {mean_val_loss:.4f} ± {std_val_loss:.4f}\")\n",
    "        \n",
    "        # Armazenar os resultados\n",
    "        avaliacao_resultados.append({\n",
    "            'Canal_Alvo': canal_alvo,\n",
    "            'Canais_Entrada': ', '.join(canais_entrada),\n",
    "            'n_feat': n_feat,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs,\n",
    "            'Mean_Val_Loss': mean_val_loss,\n",
    "            'Std_Val_Loss': std_val_loss\n",
    "        })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "avaliacao_df = pd.DataFrame(avaliacao_resultados)\n",
    "print(\"\\nResultados da Avaliação:\")\n",
    "print(avaliacao_df)\n",
    "\n",
    "# Salvar os resultados da avaliação em um arquivo CSV\n",
    "avaliacao_df.to_csv('avaliacao_resultados.csv', index=False)\n",
    "print(\"Resultados da avaliação salvos em 'avaliacao_resultados.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Configurações gerais\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Função de penalidade do gradiente para WGAN-GP\n",
    "def gradient_penalty(discriminator, real_data, fake_data, cond, lambda_gp=10):\n",
    "    batch_size = real_data.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_data)\n",
    "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolated = interpolated.detach().requires_grad_(True)\n",
    "    \n",
    "    prob_interpolated = discriminator(interpolated, cond)\n",
    "    \n",
    "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                                    grad_outputs=torch.ones(prob_interpolated.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
    "    return gradient_penalty\n",
    "\n",
    "# Gerador Condicional\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, cond_dim, output_dim, n_features=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim + cond_dim, n_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features * 4, n_features * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features * 8, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, cond):\n",
    "        # z: (batch_size, z_dim)\n",
    "        # cond: (batch_size, cond_dim)\n",
    "        x = torch.cat([z, cond], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminador Condicional\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, n_features=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, n_features * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features * 8, n_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features * 4, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # cond: (batch_size, cond_dim)\n",
    "        x = torch.cat([x, cond], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Função de treinamento principal para WGAN-GP\n",
    "def train_wgan_gp(generator, discriminator, dataloader, num_epochs=100, z_dim=100, lambda_gp=10):\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_d_loss = 0\n",
    "        epoch_g_loss = 0\n",
    "        for batch_inputs, batch_targets in tqdm(dataloader, leave=False):\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            batch_size = batch_inputs.size(0)\n",
    "            \n",
    "            # Achatar as entradas e alvos\n",
    "            cond = batch_inputs.reshape(batch_size, -1)  # Usar reshape em vez de view\n",
    "            real_data = batch_targets.reshape(batch_size, -1)  # Usar reshape em vez de view\n",
    "            \n",
    "            # Treinamento do Discriminador\n",
    "            for _ in range(5):\n",
    "                z = torch.randn(batch_size, z_dim).to(device)\n",
    "                fake_data = generator(z, cond)\n",
    "                \n",
    "                real_validity = discriminator(real_data, cond)\n",
    "                fake_validity = discriminator(fake_data.detach(), cond)\n",
    "                \n",
    "                gp = gradient_penalty(discriminator, real_data, fake_data.detach(), cond)\n",
    "                \n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gp\n",
    "                \n",
    "                optimizer_D.zero_grad()\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                epoch_d_loss += d_loss.item()\n",
    "            \n",
    "            # Treinamento do Gerador\n",
    "            z = torch.randn(batch_size, z_dim).to(device)\n",
    "            fake_data = generator(z, cond)\n",
    "            g_loss = -torch.mean(discriminator(fake_data, cond))\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "        \n",
    "        avg_d_loss = epoch_d_loss / len(dataloader)\n",
    "        avg_g_loss = epoch_g_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "# Função para gerar dados sintéticos usando o Gerador treinado\n",
    "def generate_synthetic_data(generator, conditioning_inputs, z_dim=100):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_size = conditioning_inputs.size(0)\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        synthetic_data = generator(z, conditioning_inputs)\n",
    "    return synthetic_data.cpu().numpy()\n",
    "\n",
    "# Carregar os dados de EEG utilizando pandas\n",
    "eeg_df = pd.read_csv('bci_competition_3_v.csv')\n",
    "\n",
    "# Lista de diretrizes para criar canais sintéticos\n",
    "diretrizes = [\n",
    "    # {'canal_alvo': 'F7', 'canais_entrada': ['FC5', 'F3']},\n",
    "    # Adicione mais diretrizes conforme necessário\n",
    "    # {'canal_alvo': 'AF3', 'canais_entrada': ['Fp1', 'F3']},\n",
    "    # {'canal_alvo': 'AF4', 'canais_entrada': ['Fp2', 'F4']},\n",
    "    # {'canal_alvo': 'F8', 'canais_entrada': ['T8', 'F4']},\n",
    "    # {'canal_alvo': 'T7', 'canais_entrada': ['C3', 'CP1']},\n",
    "    # {'canal_alvo': 'T8', 'canais_entrada': ['C4', 'CP2']},\n",
    "    # {'canal_alvo': 'Fp1', 'canais_entrada': ['AF3', 'F3']},\n",
    "    {'canal_alvo': 'Fp2', 'canais_entrada': ['AF4', 'F4']}\n",
    "]\n",
    "\n",
    "# Lista para armazenar as métricas de avaliação\n",
    "avaliacao_resultados = []\n",
    "\n",
    "# Iterar sobre cada diretriz\n",
    "for diretriz in diretrizes:\n",
    "    canal_alvo = diretriz['canal_alvo']\n",
    "    canais_entrada = diretriz['canais_entrada']\n",
    "    \n",
    "    print(f\"\\nTreinando o modelo WGAN-GP para prever o canal {canal_alvo} usando {canais_entrada}\\n\")\n",
    "    \n",
    "    # Selecionar os dados dos canais de entrada e do canal alvo\n",
    "    inputs = eeg_df[canais_entrada].values\n",
    "    targets = eeg_df[[canal_alvo]].values\n",
    "\n",
    "    # Calcular a média e desvio padrão dos canais de entrada e alvo\n",
    "    mean_inputs = inputs.mean(axis=0)\n",
    "    std_inputs = inputs.std(axis=0)\n",
    "    inputs_normalizado = (inputs - mean_inputs) / std_inputs\n",
    "\n",
    "    mean_target = targets.mean(axis=0)\n",
    "    std_target = targets.std(axis=0)\n",
    "    targets_normalizado = (targets - mean_target) / std_target\n",
    "\n",
    "    # Garantir que inputs e targets tenham o mesmo número de amostras\n",
    "    min_length = min(len(inputs_normalizado), len(targets_normalizado))\n",
    "    inputs_normalizado = inputs_normalizado[:min_length]\n",
    "    targets_normalizado = targets_normalizado[:min_length]\n",
    "\n",
    "    # Preparação dos dados: Segmentar em sequências menores\n",
    "    seq_length = 128\n",
    "    num_samples = inputs_normalizado.shape[0]\n",
    "    num_segments = num_samples // seq_length\n",
    "\n",
    "    inputs_normalizado = inputs_normalizado[:num_segments * seq_length]\n",
    "    targets_normalizado = targets_normalizado[:num_segments * seq_length]\n",
    "\n",
    "    inputs_segments = inputs_normalizado.reshape(num_segments, seq_length, len(canais_entrada))\n",
    "    targets_segments = targets_normalizado.reshape(num_segments, seq_length, 1)\n",
    "\n",
    "    # Transpor para (num_segments, num_channels, seq_length)\n",
    "    inputs_segments = inputs_segments.transpose(0, 2, 1)\n",
    "    targets_segments = targets_segments.transpose(0, 2, 1)\n",
    "\n",
    "    # Criar Dataset personalizado\n",
    "    class EEGDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, targets):\n",
    "            self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            x = self.inputs[idx].reshape(-1)  # Usar reshape em vez de view\n",
    "            y = self.targets[idx].reshape(-1)  # Usar reshape em vez de view\n",
    "            return x, y\n",
    "\n",
    "    # Criar o dataset e dataloader\n",
    "    dataset = EEGDataset(inputs_segments, targets_segments)\n",
    "    batch_size = 32\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Definir as dimensões\n",
    "    z_dim = 100\n",
    "    cond_dim = inputs_segments.shape[1] * inputs_segments.shape[2]  # Número de canais de entrada * comprimento da sequência\n",
    "    output_dim = targets_segments.shape[1] * targets_segments.shape[2]  # 1 * comprimento da sequência\n",
    "\n",
    "    # Inicializar o Gerador e o Discriminador\n",
    "    generator = Generator(z_dim=z_dim, cond_dim=cond_dim, output_dim=output_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim=output_dim, cond_dim=cond_dim).to(device)\n",
    "\n",
    "    # Treinar o modelo WGAN-GP\n",
    "    num_epochs = 100  # Ajuste conforme necessário\n",
    "    train_wgan_gp(generator, discriminator, dataloader, num_epochs=num_epochs, z_dim=z_dim, lambda_gp=10)\n",
    "\n",
    "    # Salvar o modelo treinado\n",
    "    model_filename = f'generator_{canal_alvo}.pth'\n",
    "    torch.save(generator.state_dict(), model_filename)\n",
    "    print(f\"Modelo do Gerador salvo como {model_filename}\")\n",
    "\n",
    "    # Gerar o canal sintético para todos os segmentos\n",
    "    conditioning_inputs = torch.tensor(inputs_segments, dtype=torch.float32).to(device)\n",
    "    conditioning_inputs_flat = conditioning_inputs.reshape(conditioning_inputs.size(0), -1)  # Usar reshape em vez de view\n",
    "\n",
    "    # Gerar o canal sintético\n",
    "    synthetic_data = generate_synthetic_data(generator, conditioning_inputs_flat, z_dim=z_dim)\n",
    "\n",
    "    # Remodelar os dados sintéticos\n",
    "    synthetic_data = synthetic_data.reshape(-1, 1, seq_length)\n",
    "\n",
    "    # Desnormalizar os dados\n",
    "    synthetic_data_desnormalizado = synthetic_data * std_target + mean_target  # Desnormalizar\n",
    "\n",
    "    # Flatten the synthetic data\n",
    "    synthetic_channel_flat = synthetic_data_desnormalizado.reshape(-1)\n",
    "\n",
    "    # Criar uma nova coluna no eeg_df para o canal sintético\n",
    "    column_name = f'Sintetico_{canal_alvo}'\n",
    "\n",
    "    # Garantir que o comprimento corresponda aos índices em eeg_df\n",
    "    eeg_df[column_name] = np.nan  # Inicializar a nova coluna com NaNs\n",
    "    eeg_df.loc[:len(synthetic_channel_flat)-1, column_name] = synthetic_channel_flat\n",
    "\n",
    "    # Calcular o MSE e a correlação usando todos os dados\n",
    "    original_target = targets_normalizado.reshape(-1) * std_target + mean_target\n",
    "    synthetic_target = synthetic_channel_flat\n",
    "\n",
    "    mse = mean_squared_error(original_target[:len(synthetic_target)], synthetic_target)\n",
    "    corr = np.corrcoef(original_target[:len(synthetic_target)], synthetic_target)[0, 1]\n",
    "    print(f\"MSE para {canal_alvo} = {mse:.4f}, Correlação = {corr:.4f}\")\n",
    "\n",
    "    # Armazenar as métricas de avaliação\n",
    "    avaliacao_resultados.append({\n",
    "        'Canal_Alvo': canal_alvo,\n",
    "        'Canais_Entrada': ', '.join(canais_entrada),\n",
    "        'MSE': mse,\n",
    "        'Correlacao': corr\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "avaliacao_df = pd.DataFrame(avaliacao_resultados)\n",
    "print(\"\\nResultados da Avaliação:\")\n",
    "print(avaliacao_df)\n",
    "\n",
    "# Salvar os resultados da avaliação em um arquivo CSV\n",
    "avaliacao_df.to_csv(f'avaliacao_resultados_wgan_{canal_alvo}.csv', index=False)\n",
    "print(f\"Resultados da avaliação salvos em 'avaliacao_resultados_wgan_{canal_alvo}.csv'\")\n",
    "\n",
    "# Salvar o eeg_df com os dados sintéticos\n",
    "eeg_df.to_csv(f'eeg_com_sintetico_wgan_{canal_alvo}.csv', index=False)\n",
    "print(f\"Dados sintéticos adicionados e salvos em 'eeg_com_sintetico_wgan_{canal_alvo}.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do GA c/ Modelo de Difusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['patient', 'time', 'label', 'epoch', 'Fp1', 'AF3', 'F7', 'F3', 'FC1',\n",
      "       'FC5', 'T7', 'C3', 'CP1', 'CP5', 'P7', 'P3', 'Pz', 'PO3', 'O1', 'Oz',\n",
      "       'O2', 'PO4', 'P4', 'P8', 'CP6', 'CP2', 'C4', 'T8', 'FC6', 'FC2', 'F4',\n",
      "       'F8', 'AF4', 'Fp2', 'Fz', 'Cz'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/m10cj0cx4ls1q1q7t63fvykr0000gn/T/ipykernel_35967/1742261654.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  eeg_df_syntetic[channels] = eeg_df_syntetic[channels].applymap(clean_numeric_values)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tmax     \n",
      "0  \t50    \t0.344576\t0.351121\n",
      "1  \t26    \t0.345355\t0.350685\n",
      "2  \t31    \t0.346454\t0.351693\n",
      "3  \t27    \t0.348281\t0.353191\n",
      "4  \t24    \t0.349098\t0.353191\n",
      "5  \t29    \t0.349988\t0.353191\n",
      "6  \t35    \t0.350335\t0.353436\n",
      "7  \t30    \t0.350363\t0.353436\n",
      "8  \t30    \t0.351281\t0.353436\n",
      "9  \t36    \t0.351597\t0.353436\n",
      "10 \t27    \t0.352204\t0.353436\n",
      "Canais selecionados: ['Fp1', 'F7', 'T7', 'CP1', 'CP5', 'P7', 'P3', 'Pz', 'PO3', 'O1', 'O2', 'CP6', 'C4', 'T8', 'FC2', 'Fp2']\n",
      "Canais artificiais selecionados: ['F7', 'Fp1', 'Fp2', 'T7', 'T8']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Função para limpar valores inválidos em uma string numérica\n",
    "def clean_numeric_values(value):\n",
    "    # Remover caracteres indesejados como '.' e '-'\n",
    "    cleaned_value = str(value).replace('.', '').replace('-', '')\n",
    "    try:\n",
    "        return float(cleaned_value)\n",
    "    except ValueError:\n",
    "        return np.nan  # Se não for possível converter, retorna NaN\n",
    "\n",
    "# Carregar o dataset com o delimitador correto\n",
    "eeg_df_syntetic = pd.read_csv('bci_competition_3_v_syntetic_modelo_difusao.csv', delimiter=';')\n",
    "\n",
    "# Verificar as colunas disponíveis no dataset\n",
    "print(eeg_df_syntetic.columns)\n",
    "\n",
    "# Definir as colunas que são os canais no dataset (ajuste conforme necessário)\n",
    "channels = eeg_df_syntetic.columns[4:]  # Ajuste o índice se necessário\n",
    "\n",
    "# Aplicar a função de limpeza a todas as colunas de canais\n",
    "eeg_df_syntetic[channels] = eeg_df_syntetic[channels].applymap(clean_numeric_values)\n",
    "\n",
    "# Converter as colunas de canais para números (substituir valores inválidos por NaN)\n",
    "eeg_df_syntetic[channels] = eeg_df_syntetic[channels].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Preencher valores NaN com a média das colunas\n",
    "eeg_df_syntetic[channels] = eeg_df_syntetic[channels].fillna(eeg_df_syntetic[channels].mean())\n",
    "\n",
    "# Definir variáveis independentes e a variável alvo\n",
    "X = eeg_df_syntetic[channels]\n",
    "y = eeg_df_syntetic['label']  # Assumindo que a coluna 'label' contém as classes, ajuste se necessário\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Configurações do Algoritmo Genético\n",
    "num_channels = len(channels)\n",
    "\n",
    "# Criar classes do GA\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Função para criar um indivíduo (seleção de canais)\n",
    "def create_individual():\n",
    "    individual = [random.randint(0, 1) for _ in range(num_channels)]\n",
    "    while sum(individual) == 0:\n",
    "        individual = [random.randint(0, 1) for _ in range(num_channels)]\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "# Função para avaliar o indivíduo\n",
    "def evaluate(individual):\n",
    "    selected_channels = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_channels) == 0:\n",
    "        return 0,\n",
    "    X_train_selected = X_train_combined.iloc[:, selected_channels]\n",
    "    X_test_selected = X_test_combined.iloc[:, selected_channels]\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    classifier.fit(X_train_selected, y_train)\n",
    "    y_pred = classifier.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy,\n",
    "\n",
    "# Configurar a toolbox do GA\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Definir parâmetros do GA\n",
    "population_size = 50\n",
    "num_generations = 10\n",
    "mutation_prob = 0.2\n",
    "crossover_prob = 0.5\n",
    "\n",
    "# Criar a população inicial\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Estatísticas\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "# Executar o Algoritmo Genético\n",
    "population, logbook = algorithms.eaSimple(population, toolbox,\n",
    "                                          cxpb=crossover_prob,\n",
    "                                          mutpb=mutation_prob,\n",
    "                                          ngen=num_generations,\n",
    "                                          stats=stats,\n",
    "                                          verbose=True)\n",
    "\n",
    "# Análise dos resultados\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_channels_indices = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "selected_channels = [channels[i] for i in selected_channels_indices]\n",
    "\n",
    "print(f\"Canais selecionados: {selected_channels}\")\n",
    "\n",
    "# Identificar se canais sintéticos foram selecionados\n",
    "channels_to_remove = ['AF3', 'AF4', 'F7', 'F8', 'Fp1', 'Fp2', 'T7', 'T8']  # Exemplos de canais sintéticos gerados\n",
    "artificial_channels_selected = [ch for ch in channels_to_remove if ch in selected_channels]\n",
    "print(f\"Canais artificiais selecionados: {artificial_channels_selected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores (Regressão Logistica, KNN, CNN e U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Modelo de Difusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando e avaliando o classificador: Regressão Logística\n",
      "\n",
      "Regressão Logística - Dados Originais\n",
      "Acurácia: 0.3714\n",
      "Precisão: 0.1845\n",
      "Recall: 0.3714\n",
      "F1-Score: 0.2295\n",
      "Matriz de Confusão:\n",
      "[[   0    1 6425]\n",
      " [   0   92 7834]\n",
      " [   0 1125 8997]]\n",
      "\n",
      "Regressão Logística - Dados Sintéticos\n",
      "Acurácia: 0.3924\n",
      "Precisão: 0.3287\n",
      "Recall: 0.3924\n",
      "F1-Score: 0.2888\n",
      "Matriz de Confusão:\n",
      "[[ 162  387 5877]\n",
      " [ 326  774 6826]\n",
      " [ 432 1023 8667]]\n",
      "\n",
      "Treinando e avaliando o classificador: KNN\n",
      "\n",
      "KNN - Dados Originais\n",
      "Acurácia: 0.9307\n",
      "Precisão: 0.9320\n",
      "Recall: 0.9307\n",
      "F1-Score: 0.9309\n",
      "Matriz de Confusão:\n",
      "[[6085  196  145]\n",
      " [ 331 7402  193]\n",
      " [ 379  452 9291]]\n",
      "\n",
      "KNN - Dados Sintéticos\n",
      "Acurácia: 0.4058\n",
      "Precisão: 0.4208\n",
      "Recall: 0.4058\n",
      "F1-Score: 0.4088\n",
      "Matriz de Confusão:\n",
      "[[2847 1895 1684]\n",
      " [2663 2987 2276]\n",
      " [3056 2968 4098]]\n",
      "\n",
      "Treinando e avaliando o classificador: CNN\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
      "\n",
      "CNN - Dados Originais\n",
      "Acurácia: 0.9889\n",
      "Precisão: 0.9890\n",
      "Recall: 0.9889\n",
      "F1-Score: 0.9889\n",
      "Matriz de Confusão:\n",
      "[[ 6300   110    16]\n",
      " [    3  7862    61]\n",
      " [   18    63 10041]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step\n",
      "\n",
      "CNN - Dados Sintéticos\n",
      "Acurácia: 0.9572\n",
      "Precisão: 0.9575\n",
      "Recall: 0.9572\n",
      "F1-Score: 0.9571\n",
      "Matriz de Confusão:\n",
      "[[5971  176  279]\n",
      " [   9 7690  227]\n",
      " [ 125  232 9765]]\n",
      "\n",
      "Treinando e avaliando o classificador: U-Net\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\n",
      "U-Net - Dados Originais\n",
      "Acurácia: 0.9879\n",
      "Precisão: 0.9879\n",
      "Recall: 0.9879\n",
      "F1-Score: 0.9879\n",
      "Matriz de Confusão:\n",
      "[[6401   16    9]\n",
      " [  34 7852   40]\n",
      " [  77  121 9924]]\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      "U-Net - Dados Sintéticos\n",
      "Acurácia: 0.9631\n",
      "Precisão: 0.9633\n",
      "Recall: 0.9631\n",
      "F1-Score: 0.9631\n",
      "Matriz de Confusão:\n",
      "[[6141  114  171]\n",
      " [  69 7574  283]\n",
      " [  56  210 9856]]\n",
      "\n",
      "Tabela de Resultados:\n",
      "         Classificador    Dataset  Acurácia  Precisão    Recall  F1-Score\n",
      "0  Regressão Logística   Original  0.371374  0.184463  0.371374  0.229478\n",
      "1  Regressão Logística  Sintético  0.392376  0.328742  0.392376  0.288814\n",
      "2                  KNN   Original  0.930702  0.931978  0.930702  0.930891\n",
      "3                  KNN  Sintético  0.405818  0.420828  0.405818  0.408811\n",
      "4                  CNN   Original  0.988927  0.989007  0.988927  0.988935\n",
      "5                  CNN  Sintético  0.957179  0.957545  0.957179  0.957141\n",
      "6                U-Net   Original  0.987865  0.987939  0.987865  0.987863\n",
      "7                U-Net  Sintético  0.963104  0.963265  0.963104  0.963109\n",
      "\n",
      "Os resultados foram salvos em 'resultados_classificacao.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, MaxPooling1D, Flatten, Input,\n",
    "    UpSampling1D, concatenate, ZeroPadding1D, Cropping1D\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class EEGDataLoader:\n",
    "    def __init__(self, original_file, synthetic_file, label_column='label'):\n",
    "        self.original_file = original_file\n",
    "        self.synthetic_file = synthetic_file\n",
    "        self.label_column = label_column\n",
    "\n",
    "    def load_data(self):\n",
    "        # Carregar o dataset original\n",
    "        df_original = pd.read_csv(self.original_file)\n",
    "        # Carregar o dataset sintético com separador ';' e ajustando decimal e milhares\n",
    "        df_sintetico = pd.read_csv(\n",
    "            self.synthetic_file, sep=';', decimal=',', thousands='.'\n",
    "        )\n",
    "        # Remover linhas com valores ausentes\n",
    "        df_original = df_original.dropna()\n",
    "        df_sintetico = df_sintetico.dropna()\n",
    "        return df_original, df_sintetico\n",
    "\n",
    "    def prepare_features_and_labels(self, df):\n",
    "        X = df.drop(columns=[self.label_column])\n",
    "        y = df[self.label_column]\n",
    "        return X, y\n",
    "\n",
    "class EEGClassifier:\n",
    "    def __init__(self, classifier, classifier_name):\n",
    "        self.classifier = classifier\n",
    "        self.classifier_name = classifier_name\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_keras_model = isinstance(classifier, tf.keras.Model)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        if self.is_keras_model:\n",
    "            # Escalonar os dados mantendo a forma\n",
    "            nsamples, nx, ny = X_train.shape\n",
    "            X_train_flat = X_train.reshape((nsamples, nx * ny))\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train_flat)\n",
    "            X_train_scaled = X_train_scaled.reshape((nsamples, nx, ny))\n",
    "            # Converter rótulos para categóricos\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "            # Compilar e treinar o modelo\n",
    "            self.classifier.compile(\n",
    "                optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\n",
    "            )\n",
    "            self.classifier.fit(\n",
    "                X_train_scaled, y_train_categorical, epochs=10, batch_size=32, verbose=0\n",
    "            )\n",
    "        else:\n",
    "            # Escalonar os dados normalmente\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            self.classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.is_keras_model:\n",
    "            nsamples, nx, ny = X_test.shape\n",
    "            X_test_flat = X_test.reshape((nsamples, nx * ny))\n",
    "            X_test_scaled = self.scaler.transform(X_test_flat)\n",
    "            X_test_scaled = X_test_scaled.reshape((nsamples, nx, ny))\n",
    "            y_pred_proba = self.classifier.predict(X_test_scaled)\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "            return y_pred\n",
    "        else:\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            return self.classifier.predict(X_test_scaled)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        acuracia = accuracy_score(y_test, y_pred)\n",
    "        matriz_confusao = confusion_matrix(y_test, y_pred)\n",
    "        precisao = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        return {\n",
    "            'Acurácia': acuracia,\n",
    "            'Precisão': precisao,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Matriz de Confusão': matriz_confusao\n",
    "        }\n",
    "\n",
    "# Funções para criar os modelos CNN e U-Net\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_unet_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Encoder\n",
    "    conv1 = Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    pool1 = MaxPooling1D(pool_size=2, padding='same')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    pool2 = MaxPooling1D(pool_size=2, padding='same')(conv2)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv3 = Conv1D(256, 3, activation='relu', padding='same')(pool2)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling1D(size=2)(conv3)\n",
    "    # Ajuste de dimensões antes da concatenação\n",
    "    diff4 = conv2.shape[1] - up4.shape[1]\n",
    "    if diff4 > 0:\n",
    "        up4 = ZeroPadding1D(padding=(0, diff4))(up4)\n",
    "    elif diff4 < 0:\n",
    "        up4 = Cropping1D(cropping=(0, -diff4))(up4)\n",
    "    merge4 = concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = Conv1D(128, 3, activation='relu', padding='same')(merge4)\n",
    "\n",
    "    up5 = UpSampling1D(size=2)(conv4)\n",
    "    # Ajuste de dimensões antes da concatenação\n",
    "    diff5 = conv1.shape[1] - up5.shape[1]\n",
    "    if diff5 > 0:\n",
    "        up5 = ZeroPadding1D(padding=(0, diff5))(up5)\n",
    "    elif diff5 < 0:\n",
    "        up5 = Cropping1D(cropping=(0, -diff5))(up5)\n",
    "    merge5 = concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = Conv1D(64, 3, activation='relu', padding='same')(merge5)\n",
    "\n",
    "    flat = Flatten()(conv5)\n",
    "    outputs = Dense(num_classes, activation='softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Uso das classes e funções\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # Inicializar o carregador de dados\n",
    "    data_loader = EEGDataLoader(\n",
    "        original_file='bci_competition_3_v.csv',\n",
    "        synthetic_file='bci_competition_3_v_syntetic_modelo_difusao.csv',\n",
    "        label_column='label'\n",
    "    )\n",
    "    df_original, df_sintetico = data_loader.load_data()\n",
    "\n",
    "    # Preparar características e rótulos para os dados originais\n",
    "    X_original, y_original = data_loader.prepare_features_and_labels(df_original)\n",
    "    # Codificar os rótulos\n",
    "    le = LabelEncoder()\n",
    "    y_original_encoded = le.fit_transform(y_original)\n",
    "\n",
    "    # Dividir os dados em conjuntos de treinamento e teste\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_original, y_original_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Preparar características e rótulos para os dados sintéticos\n",
    "    X_sintetico, y_sintetico = data_loader.prepare_features_and_labels(df_sintetico)\n",
    "    y_sintetico_encoded = le.transform(y_sintetico)\n",
    "    X_train_sint, X_test_sint, y_train_sint, y_test_sint = train_test_split(\n",
    "        X_sintetico, y_sintetico_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Determinar o número de classes e input_shape\n",
    "    num_classes = len(le.classes_)\n",
    "    input_shape = (X_train_orig.shape[1], 1)  # Para Conv1D, input_shape é (timesteps, features)\n",
    "\n",
    "    # Reshape dos dados para modelos Keras\n",
    "    X_train_orig_reshaped = X_train_orig.values.reshape(-1, X_train_orig.shape[1], 1)\n",
    "    X_test_orig_reshaped = X_test_orig.values.reshape(-1, X_test_orig.shape[1], 1)\n",
    "\n",
    "    X_train_sint_reshaped = X_train_sint.values.reshape(-1, X_train_sint.shape[1], 1)\n",
    "    X_test_sint_reshaped = X_test_sint.values.reshape(-1, X_test_sint.shape[1], 1)\n",
    "\n",
    "    # Lista de classificadores para testar\n",
    "    classifiers = [\n",
    "        (LogisticRegression(max_iter=1000), 'Regressão Logística'),\n",
    "        (KNeighborsClassifier(n_neighbors=5), 'KNN'),\n",
    "        (create_cnn_model(input_shape=input_shape, num_classes=num_classes), 'CNN'),\n",
    "        (create_unet_model(input_shape=input_shape, num_classes=num_classes), 'U-Net')\n",
    "    ]\n",
    "\n",
    "    # Lista para armazenar os resultados\n",
    "    lista_resultados = []\n",
    "\n",
    "    # Avaliar cada classificador nos dois datasets\n",
    "    for clf, clf_name in classifiers:\n",
    "        print(f\"\\nTreinando e avaliando o classificador: {clf_name}\")\n",
    "\n",
    "        # Dados Originais\n",
    "        if clf_name in ['CNN', 'U-Net']:\n",
    "            eeg_clf_orig = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_orig.train(X_train_orig_reshaped, y_train_orig)\n",
    "            metrics_orig = eeg_clf_orig.evaluate(X_test_orig_reshaped, y_test_orig)\n",
    "        else:\n",
    "            eeg_clf_orig = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_orig.train(X_train_orig, y_train_orig)\n",
    "            metrics_orig = eeg_clf_orig.evaluate(X_test_orig, y_test_orig)\n",
    "\n",
    "        lista_resultados.append({\n",
    "            'Classificador': clf_name,\n",
    "            'Dataset': 'Original',\n",
    "            'Acurácia': metrics_orig['Acurácia'],\n",
    "            'Precisão': metrics_orig['Precisão'],\n",
    "            'Recall': metrics_orig['Recall'],\n",
    "            'F1-Score': metrics_orig['F1-Score']\n",
    "        })\n",
    "        print(f\"\\n{clf_name} - Dados Originais\")\n",
    "        print(f\"Acurácia: {metrics_orig['Acurácia']:.4f}\")\n",
    "        print(f\"Precisão: {metrics_orig['Precisão']:.4f}\")\n",
    "        print(f\"Recall: {metrics_orig['Recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics_orig['F1-Score']:.4f}\")\n",
    "        print(f\"Matriz de Confusão:\\n{metrics_orig['Matriz de Confusão']}\")\n",
    "\n",
    "        # Dados Sintéticos\n",
    "        if clf_name in ['CNN', 'U-Net']:\n",
    "            if clf_name == 'CNN':\n",
    "                clf_sint = create_cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "            else:\n",
    "                clf_sint = create_unet_model(input_shape=input_shape, num_classes=num_classes)\n",
    "            eeg_clf_sint = EEGClassifier(clf_sint, clf_name)\n",
    "            eeg_clf_sint.train(X_train_sint_reshaped, y_train_sint)\n",
    "            metrics_sint = eeg_clf_sint.evaluate(X_test_sint_reshaped, y_test_sint)\n",
    "        else:\n",
    "            eeg_clf_sint = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_sint.train(X_train_sint, y_train_sint)\n",
    "            metrics_sint = eeg_clf_sint.evaluate(X_test_sint, y_test_sint)\n",
    "\n",
    "        lista_resultados.append({\n",
    "            'Classificador': clf_name,\n",
    "            'Dataset': 'Sintético',\n",
    "            'Acurácia': metrics_sint['Acurácia'],\n",
    "            'Precisão': metrics_sint['Precisão'],\n",
    "            'Recall': metrics_sint['Recall'],\n",
    "            'F1-Score': metrics_sint['F1-Score']\n",
    "        })\n",
    "        print(f\"\\n{clf_name} - Dados Sintéticos\")\n",
    "        print(f\"Acurácia: {metrics_sint['Acurácia']:.4f}\")\n",
    "        print(f\"Precisão: {metrics_sint['Precisão']:.4f}\")\n",
    "        print(f\"Recall: {metrics_sint['Recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics_sint['F1-Score']:.4f}\")\n",
    "        print(f\"Matriz de Confusão:\\n{metrics_sint['Matriz de Confusão']}\")\n",
    "\n",
    "    # Converter a lista de resultados em um DataFrame\n",
    "    resultados = pd.DataFrame(lista_resultados)\n",
    "\n",
    "    # Exibir a tabela de resultados\n",
    "    print(\"\\nTabela de Resultados:\")\n",
    "    print(resultados)\n",
    "\n",
    "    # Opcional: Salvar os resultados em um arquivo CSV\n",
    "    resultados.to_csv('resultados_classificacao.csv', index=False)\n",
    "    print(\"\\nOs resultados foram salvos em 'resultados_classificacao.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com WGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando e avaliando o classificador: Regressão Logística\n",
      "\n",
      "Regressão Logística - Dados Originais\n",
      "Acurácia: 0.3714\n",
      "Precisão: 0.1845\n",
      "Recall: 0.3714\n",
      "F1-Score: 0.2295\n",
      "Matriz de Confusão:\n",
      "[[   0    1 6425]\n",
      " [   0   92 7834]\n",
      " [   0 1125 8997]]\n",
      "\n",
      "Regressão Logística - Dados Sintéticos\n",
      "Acurácia: 0.3982\n",
      "Precisão: 0.2947\n",
      "Recall: 0.3982\n",
      "F1-Score: 0.2782\n",
      "Matriz de Confusão:\n",
      "[[   6  335 6085]\n",
      " [  21  741 7164]\n",
      " [  87 1037 8998]]\n",
      "\n",
      "Treinando e avaliando o classificador: KNN\n",
      "\n",
      "KNN - Dados Originais\n",
      "Acurácia: 0.9307\n",
      "Precisão: 0.9320\n",
      "Recall: 0.9307\n",
      "F1-Score: 0.9309\n",
      "Matriz de Confusão:\n",
      "[[6085  196  145]\n",
      " [ 331 7402  193]\n",
      " [ 379  452 9291]]\n",
      "\n",
      "KNN - Dados Sintéticos\n",
      "Acurácia: 0.4148\n",
      "Precisão: 0.4302\n",
      "Recall: 0.4148\n",
      "F1-Score: 0.4175\n",
      "Matriz de Confusão:\n",
      "[[2973 1841 1612]\n",
      " [2644 3059 2223]\n",
      " [3008 2993 4121]]\n",
      "\n",
      "Treinando e avaliando o classificador: CNN\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\n",
      "CNN - Dados Originais\n",
      "Acurácia: 0.9881\n",
      "Precisão: 0.9881\n",
      "Recall: 0.9881\n",
      "F1-Score: 0.9881\n",
      "Matriz de Confusão:\n",
      "[[ 6361    29    36]\n",
      " [   15  7808   103]\n",
      " [   66    43 10013]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step\n",
      "\n",
      "CNN - Dados Sintéticos\n",
      "Acurácia: 0.9527\n",
      "Precisão: 0.9534\n",
      "Recall: 0.9527\n",
      "F1-Score: 0.9527\n",
      "Matriz de Confusão:\n",
      "[[6311   11  104]\n",
      " [ 379 7316  231]\n",
      " [ 153  279 9690]]\n",
      "\n",
      "Treinando e avaliando o classificador: U-Net\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\n",
      "U-Net - Dados Originais\n",
      "Acurácia: 0.9888\n",
      "Precisão: 0.9888\n",
      "Recall: 0.9888\n",
      "F1-Score: 0.9888\n",
      "Matriz de Confusão:\n",
      "[[6352   35   39]\n",
      " [  13 7878   35]\n",
      " [  78   75 9969]]\n",
      "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\n",
      "U-Net - Dados Sintéticos\n",
      "Acurácia: 0.9514\n",
      "Precisão: 0.9521\n",
      "Recall: 0.9514\n",
      "F1-Score: 0.9514\n",
      "Matriz de Confusão:\n",
      "[[6097   28  301]\n",
      " [ 122 7364  440]\n",
      " [  47  251 9824]]\n",
      "\n",
      "Tabela de Resultados:\n",
      "         Classificador    Dataset  Acurácia  Precisão    Recall  F1-Score\n",
      "0  Regressão Logística   Original  0.371374  0.184463  0.371374  0.229478\n",
      "1  Regressão Logística  Sintético  0.398178  0.294667  0.398178  0.278227\n",
      "2                  KNN   Original  0.930702  0.931978  0.930702  0.930891\n",
      "3                  KNN  Sintético  0.414848  0.430242  0.414848  0.417536\n",
      "4                  CNN   Original  0.988069  0.988077  0.988069  0.988069\n",
      "5                  CNN  Sintético  0.952725  0.953419  0.952725  0.952686\n",
      "6                U-Net   Original  0.988764  0.988784  0.988764  0.988762\n",
      "7                U-Net  Sintético  0.951418  0.952089  0.951418  0.951432\n",
      "\n",
      "Os resultados foram salvos em 'resultados_classificacao.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, MaxPooling1D, Flatten, Input,\n",
    "    UpSampling1D, concatenate, ZeroPadding1D, Cropping1D\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class EEGDataLoader:\n",
    "    def __init__(self, original_file, synthetic_file, label_column='label'):\n",
    "        self.original_file = original_file\n",
    "        self.synthetic_file = synthetic_file\n",
    "        self.label_column = label_column\n",
    "\n",
    "    def load_data(self):\n",
    "        # Carregar o dataset original\n",
    "        df_original = pd.read_csv(self.original_file)\n",
    "        # Carregar o dataset sintético com separador ';' e ajustando decimal e milhares\n",
    "        df_sintetico = pd.read_csv(\n",
    "            self.synthetic_file, sep=';', decimal=',', thousands='.'\n",
    "        )\n",
    "        # Remover linhas com valores ausentes\n",
    "        df_original = df_original.dropna()\n",
    "        df_sintetico = df_sintetico.dropna()\n",
    "        return df_original, df_sintetico\n",
    "\n",
    "    def prepare_features_and_labels(self, df):\n",
    "        X = df.drop(columns=[self.label_column])\n",
    "        y = df[self.label_column]\n",
    "        return X, y\n",
    "\n",
    "class EEGClassifier:\n",
    "    def __init__(self, classifier, classifier_name):\n",
    "        self.classifier = classifier\n",
    "        self.classifier_name = classifier_name\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_keras_model = isinstance(classifier, tf.keras.Model)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        if self.is_keras_model:\n",
    "            # Escalonar os dados mantendo a forma\n",
    "            nsamples, nx, ny = X_train.shape\n",
    "            X_train_flat = X_train.reshape((nsamples, nx * ny))\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train_flat)\n",
    "            X_train_scaled = X_train_scaled.reshape((nsamples, nx, ny))\n",
    "            # Converter rótulos para categóricos\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "            # Compilar e treinar o modelo\n",
    "            self.classifier.compile(\n",
    "                optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\n",
    "            )\n",
    "            self.classifier.fit(\n",
    "                X_train_scaled, y_train_categorical, epochs=10, batch_size=32, verbose=0\n",
    "            )\n",
    "        else:\n",
    "            # Escalonar os dados normalmente\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            self.classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.is_keras_model:\n",
    "            nsamples, nx, ny = X_test.shape\n",
    "            X_test_flat = X_test.reshape((nsamples, nx * ny))\n",
    "            X_test_scaled = self.scaler.transform(X_test_flat)\n",
    "            X_test_scaled = X_test_scaled.reshape((nsamples, nx, ny))\n",
    "            y_pred_proba = self.classifier.predict(X_test_scaled)\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "            return y_pred\n",
    "        else:\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            return self.classifier.predict(X_test_scaled)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        acuracia = accuracy_score(y_test, y_pred)\n",
    "        matriz_confusao = confusion_matrix(y_test, y_pred)\n",
    "        precisao = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        return {\n",
    "            'Acurácia': acuracia,\n",
    "            'Precisão': precisao,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Matriz de Confusão': matriz_confusao\n",
    "        }\n",
    "\n",
    "# Funções para criar os modelos CNN e U-Net\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_unet_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Encoder\n",
    "    conv1 = Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    pool1 = MaxPooling1D(pool_size=2, padding='same')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    pool2 = MaxPooling1D(pool_size=2, padding='same')(conv2)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv3 = Conv1D(256, 3, activation='relu', padding='same')(pool2)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling1D(size=2)(conv3)\n",
    "    # Ajuste de dimensões antes da concatenação\n",
    "    diff4 = conv2.shape[1] - up4.shape[1]\n",
    "    if diff4 > 0:\n",
    "        up4 = ZeroPadding1D(padding=(0, diff4))(up4)\n",
    "    elif diff4 < 0:\n",
    "        up4 = Cropping1D(cropping=(0, -diff4))(up4)\n",
    "    merge4 = concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = Conv1D(128, 3, activation='relu', padding='same')(merge4)\n",
    "\n",
    "    up5 = UpSampling1D(size=2)(conv4)\n",
    "    # Ajuste de dimensões antes da concatenação\n",
    "    diff5 = conv1.shape[1] - up5.shape[1]\n",
    "    if diff5 > 0:\n",
    "        up5 = ZeroPadding1D(padding=(0, diff5))(up5)\n",
    "    elif diff5 < 0:\n",
    "        up5 = Cropping1D(cropping=(0, -diff5))(up5)\n",
    "    merge5 = concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = Conv1D(64, 3, activation='relu', padding='same')(merge5)\n",
    "\n",
    "    flat = Flatten()(conv5)\n",
    "    outputs = Dense(num_classes, activation='softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Uso das classes e funções\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # Inicializar o carregador de dados\n",
    "    data_loader = EEGDataLoader(\n",
    "        original_file='bci_competition_3_v.csv',\n",
    "        synthetic_file='bci_competition_3_v_syntetic_wgan.csv',\n",
    "        label_column='label'\n",
    "    )\n",
    "    df_original, df_sintetico = data_loader.load_data()\n",
    "\n",
    "    # Preparar características e rótulos para os dados originais\n",
    "    X_original, y_original = data_loader.prepare_features_and_labels(df_original)\n",
    "    # Codificar os rótulos\n",
    "    le = LabelEncoder()\n",
    "    y_original_encoded = le.fit_transform(y_original)\n",
    "\n",
    "    # Dividir os dados em conjuntos de treinamento e teste\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_original, y_original_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Preparar características e rótulos para os dados sintéticos\n",
    "    X_sintetico, y_sintetico = data_loader.prepare_features_and_labels(df_sintetico)\n",
    "    y_sintetico_encoded = le.transform(y_sintetico)\n",
    "    X_train_sint, X_test_sint, y_train_sint, y_test_sint = train_test_split(\n",
    "        X_sintetico, y_sintetico_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Determinar o número de classes e input_shape\n",
    "    num_classes = len(le.classes_)\n",
    "    input_shape = (X_train_orig.shape[1], 1)  # Para Conv1D, input_shape é (timesteps, features)\n",
    "\n",
    "    # Reshape dos dados para modelos Keras\n",
    "    X_train_orig_reshaped = X_train_orig.values.reshape(-1, X_train_orig.shape[1], 1)\n",
    "    X_test_orig_reshaped = X_test_orig.values.reshape(-1, X_test_orig.shape[1], 1)\n",
    "\n",
    "    X_train_sint_reshaped = X_train_sint.values.reshape(-1, X_train_sint.shape[1], 1)\n",
    "    X_test_sint_reshaped = X_test_sint.values.reshape(-1, X_test_sint.shape[1], 1)\n",
    "\n",
    "    # Lista de classificadores para testar\n",
    "    classifiers = [\n",
    "        (LogisticRegression(max_iter=1000), 'Regressão Logística'),\n",
    "        (KNeighborsClassifier(n_neighbors=5), 'KNN'),\n",
    "        (create_cnn_model(input_shape=input_shape, num_classes=num_classes), 'CNN'),\n",
    "        (create_unet_model(input_shape=input_shape, num_classes=num_classes), 'U-Net')\n",
    "    ]\n",
    "\n",
    "    # Lista para armazenar os resultados\n",
    "    lista_resultados = []\n",
    "\n",
    "    # Avaliar cada classificador nos dois datasets\n",
    "    for clf, clf_name in classifiers:\n",
    "        print(f\"\\nTreinando e avaliando o classificador: {clf_name}\")\n",
    "\n",
    "        # Dados Originais\n",
    "        if clf_name in ['CNN', 'U-Net']:\n",
    "            eeg_clf_orig = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_orig.train(X_train_orig_reshaped, y_train_orig)\n",
    "            metrics_orig = eeg_clf_orig.evaluate(X_test_orig_reshaped, y_test_orig)\n",
    "        else:\n",
    "            eeg_clf_orig = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_orig.train(X_train_orig, y_train_orig)\n",
    "            metrics_orig = eeg_clf_orig.evaluate(X_test_orig, y_test_orig)\n",
    "\n",
    "        lista_resultados.append({\n",
    "            'Classificador': clf_name,\n",
    "            'Dataset': 'Original',\n",
    "            'Acurácia': metrics_orig['Acurácia'],\n",
    "            'Precisão': metrics_orig['Precisão'],\n",
    "            'Recall': metrics_orig['Recall'],\n",
    "            'F1-Score': metrics_orig['F1-Score']\n",
    "        })\n",
    "        print(f\"\\n{clf_name} - Dados Originais\")\n",
    "        print(f\"Acurácia: {metrics_orig['Acurácia']:.4f}\")\n",
    "        print(f\"Precisão: {metrics_orig['Precisão']:.4f}\")\n",
    "        print(f\"Recall: {metrics_orig['Recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics_orig['F1-Score']:.4f}\")\n",
    "        print(f\"Matriz de Confusão:\\n{metrics_orig['Matriz de Confusão']}\")\n",
    "\n",
    "        # Dados Sintéticos\n",
    "        if clf_name in ['CNN', 'U-Net']:\n",
    "            if clf_name == 'CNN':\n",
    "                clf_sint = create_cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "            else:\n",
    "                clf_sint = create_unet_model(input_shape=input_shape, num_classes=num_classes)\n",
    "            eeg_clf_sint = EEGClassifier(clf_sint, clf_name)\n",
    "            eeg_clf_sint.train(X_train_sint_reshaped, y_train_sint)\n",
    "            metrics_sint = eeg_clf_sint.evaluate(X_test_sint_reshaped, y_test_sint)\n",
    "        else:\n",
    "            eeg_clf_sint = EEGClassifier(clf, clf_name)\n",
    "            eeg_clf_sint.train(X_train_sint, y_train_sint)\n",
    "            metrics_sint = eeg_clf_sint.evaluate(X_test_sint, y_test_sint)\n",
    "\n",
    "        lista_resultados.append({\n",
    "            'Classificador': clf_name,\n",
    "            'Dataset': 'Sintético',\n",
    "            'Acurácia': metrics_sint['Acurácia'],\n",
    "            'Precisão': metrics_sint['Precisão'],\n",
    "            'Recall': metrics_sint['Recall'],\n",
    "            'F1-Score': metrics_sint['F1-Score']\n",
    "        })\n",
    "        print(f\"\\n{clf_name} - Dados Sintéticos\")\n",
    "        print(f\"Acurácia: {metrics_sint['Acurácia']:.4f}\")\n",
    "        print(f\"Precisão: {metrics_sint['Precisão']:.4f}\")\n",
    "        print(f\"Recall: {metrics_sint['Recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics_sint['F1-Score']:.4f}\")\n",
    "        print(f\"Matriz de Confusão:\\n{metrics_sint['Matriz de Confusão']}\")\n",
    "\n",
    "    # Converter a lista de resultados em um DataFrame\n",
    "    resultados = pd.DataFrame(lista_resultados)\n",
    "\n",
    "    # Exibir a tabela de resultados\n",
    "    print(\"\\nTabela de Resultados:\")\n",
    "    print(resultados)\n",
    "\n",
    "    # Opcional: Salvar os resultados em um arquivo CSV\n",
    "    resultados.to_csv('resultados_classificacao_wgan.csv', index=False)\n",
    "    print(\"\\nOs resultados foram salvos em 'resultados_classificacao_wgan.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
